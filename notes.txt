5.31

manual seed 41
in_features = 214, h1 = 40, h2 = 40, out_features=2
lr = 0.01
epochs = 100
407/748 correct - 0.5441176470588235

80, 80
376/748 correct - 0.5026737967914439

20, 20
409/748 correct - 0.5467914438502673

10, 10
399/748 correct - 0.5334224598930482

20, 20, 20
403/748 correct - 0.5387700534759359

40, 20, 10
430/748 correct - 0.5748663101604278

20, 20, 10
376/748 correct - 0.5026737967914439


80, 20, 10
376/748 correct - 0.5026737967914439

epochs -> 200
lr -> 0.02
40, 20, 10
426/748 correct - 0.56951871657754

epochs -> 500
419/748 correct - 0.5601604278074866

lr -> 0.03
epochs -> 1000
814/1496 correct - 0.5441176470588235

battle log = 123K battles
lr -> 0.001
epochs = 2000


epochs -> 100
lr -> 0.1
1397/2469 correct - 0.5658161198865937

epochs -> 200
1392/2469 correct - 0.5637910085054678

6.3
NEW INFO: Less epochs is better. There's already so much training data that it doesn't help to go back 
Also, when I run inference using example battles that should have obvious outcomes, it confidently answers wrongly.
    Could be some logic error that I made in my code.

6.4 - Version 2
Created new neural network structure.
The goal of this structure is to provide consistent results. An issue I discovered while testing was that if you swap the teams between
left and right, the model answers differently than you would expect.
What I did: 
1. Split the left_team, right_team and map apart from each other
2. Passed each team through a shared layer, which treats them the same
3. Combine the map and the teams and push them through a combined layer
4. Pass through one more auxiliary layer
5. Output

Immediately, it provides a better success rate than I was able to accomplish with a simpler structure:
856/1496 correct - 0.5721925133689839

This was with the following settings:
team_neurons = 40, combined_neurons = 10, auxiliary_neurons = 10
epochs = 100
lr = 0.01


Now, I want to experiment with different activation functions. Intuitively, ReLU doesn't make sense to me in this application. 
It seems like negative values should be allowed. ChatGPT strongly advises usign some activation function, insisting that they
are necessary 

Next moves: 
- Turn off ReLU, experiment with different activation functions
- Add more layers
- Edit existing layers

VERSION 3
Keeping everything the same as above, but using LeakyReLU, I get:
820/1496 correct - 0.5481283422459893

Going back to ReLU

team_neurons = 80, combined_neurons = 40, auxiliary_neurons = 10
803/1496 correct - 0.5367647058823529

IMPORTANT NOTE:
I have to clean the data. Two problems:
    - If a game has two people on the same

I MISSPELLED THE NAME MELODIE! I WROTE HER AS MELODY!
So THOUSANDS of my battles contained garbage data, because MELODIE wasn't being written into the battle.

Fixed battles.csv. Now using battles-6-4.csv
